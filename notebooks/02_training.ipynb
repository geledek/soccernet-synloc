{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoccerNet SynLoc: Training\n",
    "\n",
    "This notebook covers:\n",
    "1. Configuration setup\n",
    "2. Model initialization with pretrained backbone\n",
    "3. Training loop with progress tracking\n",
    "4. Checkpoint saving to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install if needed\n",
    "    if not os.path.exists('soccernet-synloc'):\n",
    "        !git clone https://github.com/YOUR_USERNAME/soccernet-synloc.git\n",
    "        %cd soccernet-synloc\n",
    "        !pip install -e .[dev] -q\n",
    "    \n",
    "    DATA_ROOT = Path('/content/drive/MyDrive/SoccerNet/synloc')\n",
    "    CHECKPOINT_DIR = Path('/content/drive/MyDrive/SoccerNet/checkpoints')\n",
    "else:\n",
    "    DATA_ROOT = Path('./data/synloc')\n",
    "    CHECKPOINT_DIR = Path('./checkpoints')\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Model\n",
    "    'model_variant': 'tiny',  # tiny, s, m, l\n",
    "    'num_keypoints': 2,\n",
    "    'input_size': (640, 640),\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 16,\n",
    "    'epochs': 100,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 5e-4,\n",
    "    'warmup_epochs': 5,\n",
    "    \n",
    "    # Data\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Loss weights\n",
    "    'loss_cls_weight': 1.0,\n",
    "    'loss_bbox_weight': 5.0,\n",
    "    'loss_obj_weight': 1.0,\n",
    "    'loss_kpt_weight': 1.0,\n",
    "    \n",
    "    # Augmentation\n",
    "    'use_mosaic': True,  # Enable mosaic augmentation\n",
    "    'mosaic_prob': 0.5,\n",
    "    \n",
    "    # Misc\n",
    "    'use_amp': True,  # Automatic mixed precision\n",
    "    'save_interval': 10,  # Save every N epochs\n",
    "}\n",
    "\n",
    "# Adjust for GPU memory\n",
    "if device == 'cuda':\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_mem < 8:  # T4 with 15GB, but be conservative\n",
    "        config['batch_size'] = 8\n",
    "        config['model_variant'] = 'tiny'\n",
    "    elif gpu_mem < 20:\n",
    "        config['batch_size'] = 16\n",
    "    else:  # A100\n",
    "        config['batch_size'] = 32\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.data import SynLocDataset, get_train_transforms, get_val_transforms\n",
    "\n",
    "# Training dataset\n",
    "train_transforms = get_train_transforms(\n",
    "    config['input_size'][0],\n",
    "    use_mosaic=config['use_mosaic']\n",
    ")\n",
    "\n",
    "train_dataset = SynLocDataset(\n",
    "    ann_file=str(DATA_ROOT / 'train/annotations.json'),\n",
    "    img_dir=str(DATA_ROOT / 'train/images'),\n",
    "    transforms=train_transforms,\n",
    "    input_size=config['input_size']\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val_transforms = get_val_transforms(config['input_size'][0])\n",
    "\n",
    "val_dataset = SynLocDataset(\n",
    "    ann_file=str(DATA_ROOT / 'valid/annotations.json'),\n",
    "    img_dir=str(DATA_ROOT / 'valid/images'),\n",
    "    transforms=val_transforms,\n",
    "    input_size=config['input_size']\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} images\")\n",
    "print(f\"Val dataset: {len(val_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    collate_fn=SynLocDataset.collate_fn,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    collate_fn=SynLocDataset.collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.models import YOLOXPose\n",
    "\n",
    "model = YOLOXPose(\n",
    "    variant=config['model_variant'],\n",
    "    num_keypoints=config['num_keypoints'],\n",
    "    input_size=config['input_size']\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: YOLOX-Pose {config['model_variant']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained backbone (optional)\n",
    "# You can download COCO pretrained weights and load them here\n",
    "\n",
    "pretrained_path = CHECKPOINT_DIR / f'yolox_{config[\"model_variant\"]}_coco.pth'\n",
    "\n",
    "if pretrained_path.exists():\n",
    "    print(f\"Loading pretrained weights from {pretrained_path}\")\n",
    "    state_dict = torch.load(pretrained_path, map_location='cpu')\n",
    "    \n",
    "    # Filter out head weights (different number of keypoints)\n",
    "    model_state = model.state_dict()\n",
    "    filtered_state = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k in model_state and v.shape == model_state[k].shape:\n",
    "            filtered_state[k] = v\n",
    "    \n",
    "    model.load_state_dict(filtered_state, strict=False)\n",
    "    print(f\"Loaded {len(filtered_state)}/{len(model_state)} layers\")\n",
    "else:\n",
    "    print(\"No pretrained weights found, training from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.training import SynLocTrainer\n",
    "\n",
    "trainer = SynLocTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    epochs=config['epochs'],\n",
    "    warmup_epochs=config['warmup_epochs'],\n",
    "    checkpoint_dir=str(CHECKPOINT_DIR),\n",
    "    use_amp=config['use_amp']\n",
    ")\n",
    "\n",
    "print(f\"Trainer initialized\")\n",
    "print(f\"Using AMP: {config['use_amp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Resume from checkpoint\n",
    "resume_path = None  # Set to checkpoint path to resume\n",
    "\n",
    "if resume_path and Path(resume_path).exists():\n",
    "    trainer.load_checkpoint(resume_path)\n",
    "    print(f\"Resumed from {resume_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "history = trainer.train()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training curves.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Total loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], label='Train')\n",
    "    if 'val_loss' in history:\n",
    "        axes[0, 0].plot(epochs, history['val_loss'], label='Val')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Classification loss\n",
    "    if 'train_cls_loss' in history:\n",
    "        axes[0, 1].plot(epochs, history['train_cls_loss'], label='Train')\n",
    "        if 'val_cls_loss' in history:\n",
    "            axes[0, 1].plot(epochs, history['val_cls_loss'], label='Val')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].set_title('Classification Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    # Bbox loss\n",
    "    if 'train_bbox_loss' in history:\n",
    "        axes[1, 0].plot(epochs, history['train_bbox_loss'], label='Train')\n",
    "        if 'val_bbox_loss' in history:\n",
    "            axes[1, 0].plot(epochs, history['val_bbox_loss'], label='Val')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Loss')\n",
    "        axes[1, 0].set_title('Bbox Loss')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    # Keypoint loss\n",
    "    if 'train_kpt_loss' in history:\n",
    "        axes[1, 1].plot(epochs, history['train_kpt_loss'], label='Train')\n",
    "        if 'val_kpt_loss' in history:\n",
    "            axes[1, 1].plot(epochs, history['val_kpt_loss'], label='Val')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].set_title('Keypoint Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule\n",
    "if 'lr' in history:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history['lr'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint\n",
    "final_path = CHECKPOINT_DIR / 'final_model.pth'\n",
    "trainer.save_checkpoint(str(final_path))\n",
    "print(f\"Saved final model to {final_path}\")\n",
    "\n",
    "# Save config\n",
    "import json\n",
    "config_path = CHECKPOINT_DIR / 'config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"Saved config to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.evaluation import visualize_predictions\n",
    "\n",
    "# Run inference on a few validation samples\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# Get a batch\n",
    "batch = next(iter(val_loader))\n",
    "images = batch['image'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model.predict(\n",
    "        images,\n",
    "        input_size=config['input_size'],\n",
    "        score_thr=0.3\n",
    "    )\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i >= len(images):\n",
    "        break\n",
    "    \n",
    "    # Denormalize image\n",
    "    img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img = img * std + mean\n",
    "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Draw predictions\n",
    "    vis_img = visualize_predictions(\n",
    "        img,\n",
    "        {\n",
    "            'bboxes': results[i]['bboxes'].cpu().numpy(),\n",
    "            'scores': results[i]['scores'].cpu().numpy(),\n",
    "            'keypoints': results[i]['keypoints'].cpu().numpy(),\n",
    "            'keypoint_scores': results[i]['keypoint_scores'].cpu().numpy()\n",
    "        },\n",
    "        score_thr=0.3\n",
    "    )\n",
    "    \n",
    "    ax.imshow(vis_img)\n",
    "    ax.set_title(f\"Image {i}: {len(results[i]['bboxes'])} detections\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Training complete! Key outputs:\n",
    "- Model checkpoint: `{CHECKPOINT_DIR}/final_model.pth`\n",
    "- Training config: `{CHECKPOINT_DIR}/config.json`\n",
    "\n",
    "Next steps:\n",
    "- Proceed to `03_evaluation.ipynb` for mAP-LocSim evaluation\n",
    "- Tune hyperparameters based on validation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
