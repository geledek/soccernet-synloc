{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoccerNet SynLoc: Evaluation\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading trained model\n",
    "2. mAP-LocSim evaluation\n",
    "3. Error analysis\n",
    "4. Ablation study framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    if not os.path.exists('soccernet-synloc'):\n",
    "        !git clone https://github.com/YOUR_USERNAME/soccernet-synloc.git\n",
    "        %cd soccernet-synloc\n",
    "        !pip install -e .[dev] -q\n",
    "    \n",
    "    DATA_ROOT = Path('/content/drive/MyDrive/SoccerNet/synloc')\n",
    "    CHECKPOINT_DIR = Path('/content/drive/MyDrive/SoccerNet/checkpoints')\n",
    "else:\n",
    "    DATA_ROOT = Path('./data/synloc')\n",
    "    CHECKPOINT_DIR = Path('./checkpoints')\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.models import YOLOXPose\n",
    "\n",
    "# Load config\n",
    "config_path = CHECKPOINT_DIR / 'config.json'\n",
    "if config_path.exists():\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Loaded config:\")\n",
    "    for k, v in config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    # Default config\n",
    "    config = {\n",
    "        'model_variant': 'tiny',\n",
    "        'num_keypoints': 2,\n",
    "        'input_size': (640, 640),\n",
    "        'batch_size': 16\n",
    "    }\n",
    "    print(\"Using default config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = YOLOXPose(\n",
    "    variant=config['model_variant'],\n",
    "    num_keypoints=config['num_keypoints'],\n",
    "    input_size=tuple(config['input_size'])\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "checkpoint_path = CHECKPOINT_DIR / 'final_model.pth'\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "else:\n",
    "    print(\"No checkpoint found, using random weights\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.data import SynLocDataset, get_val_transforms\n",
    "\n",
    "# Test set for evaluation\n",
    "test_dataset = SynLocDataset(\n",
    "    ann_file=str(DATA_ROOT / 'test/annotations.json'),\n",
    "    img_dir=str(DATA_ROOT / 'test/images'),\n",
    "    transforms=get_val_transforms(config['input_size'][0]),\n",
    "    input_size=tuple(config['input_size'])\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=SynLocDataset.collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.evaluation import run_inference\n",
    "\n",
    "# Run inference\n",
    "results = run_inference(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device=device,\n",
    "    score_thr=0.01,  # Low threshold to get full P-R curve\n",
    "    nms_thr=0.65,\n",
    "    max_per_img=100\n",
    ")\n",
    "\n",
    "print(f\"Total detections: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. mAP-LocSim Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.evaluation import evaluate_predictions\n",
    "\n",
    "# Evaluate with mAP-LocSim\n",
    "metrics = evaluate_predictions(\n",
    "    gt_file=str(DATA_ROOT / 'test/annotations.json'),\n",
    "    results=results,\n",
    "    position_from_keypoint_index=1,  # pelvis_ground\n",
    "    score_threshold=None  # Auto-select via F1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"=\"*50)\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.evaluation.locsim import LocSimCOCOeval\n",
    "from xtcocotools.coco import COCO\n",
    "\n",
    "# Run full evaluation to get P-R data\n",
    "coco_gt = COCO(str(DATA_ROOT / 'test/annotations.json'))\n",
    "coco_dt = coco_gt.loadRes(results)\n",
    "\n",
    "coco_eval = LocSimCOCOeval(coco_gt, coco_dt, 'bbox')\n",
    "coco_eval.params.useSegm = None\n",
    "coco_eval.params.position_from_keypoint_index = 1\n",
    "\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot P-R curve at LocSim=0.5\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision-Recall\n",
    "precision = coco_eval.eval['precision_50']\n",
    "recall = coco_eval.eval['recall_50']\n",
    "f1 = coco_eval.eval['f1_50']\n",
    "scores = coco_eval.eval['scores_50']\n",
    "\n",
    "axes[0].plot(recall, precision, 'b-', linewidth=2)\n",
    "axes[0].fill_between(recall, precision, alpha=0.2)\n",
    "axes[0].set_xlabel('Recall')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('Precision-Recall Curve @ LocSim=0.5')\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(True)\n",
    "\n",
    "# F1 vs Score threshold\n",
    "valid = scores > 0\n",
    "axes[1].plot(scores[valid], f1[valid], 'g-', linewidth=2)\n",
    "best_idx = f1.argmax()\n",
    "axes[1].axvline(x=scores[best_idx], color='r', linestyle='--', \n",
    "                label=f'Best threshold: {scores[best_idx]:.3f}')\n",
    "axes[1].set_xlabel('Score Threshold')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('F1 Score vs Score Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best F1: {f1[best_idx]:.4f} at threshold {scores[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by category\n",
    "from collections import defaultdict\n",
    "\n",
    "# Use optimal threshold\n",
    "optimal_threshold = metrics['score_threshold']\n",
    "\n",
    "# Count TP, FP, FN per image\n",
    "error_analysis = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0, 'gt_count': 0})\n",
    "\n",
    "rng = coco_eval.params.areaRng[coco_eval.params.areaRngLbl.index('all')]\n",
    "iou_idx = np.where(coco_eval.params.iouThrs == 0.5)[0][0]\n",
    "\n",
    "for e in coco_eval.evalImgs:\n",
    "    if e is None or e['aRng'] != rng:\n",
    "        continue\n",
    "    \n",
    "    img_id = e['image_id']\n",
    "    error_analysis[img_id]['gt_count'] = len(e['gtIds'])\n",
    "    \n",
    "    # Count matches\n",
    "    dt_scores = np.array(e['dtScores'])\n",
    "    dt_matches = e['dtMatches'][iou_idx]\n",
    "    \n",
    "    above_thr = dt_scores >= optimal_threshold\n",
    "    matched = dt_matches > 0\n",
    "    \n",
    "    tp = (above_thr & matched).sum()\n",
    "    fp = (above_thr & ~matched).sum()\n",
    "    fn = len(e['gtIds']) - tp\n",
    "    \n",
    "    error_analysis[img_id]['tp'] = tp\n",
    "    error_analysis[img_id]['fp'] = fp\n",
    "    error_analysis[img_id]['fn'] = max(0, fn)\n",
    "\n",
    "# Aggregate\n",
    "total_tp = sum(e['tp'] for e in error_analysis.values())\n",
    "total_fp = sum(e['fp'] for e in error_analysis.values())\n",
    "total_fn = sum(e['fn'] for e in error_analysis.values())\n",
    "\n",
    "print(f\"Error Analysis @ threshold={optimal_threshold:.3f}\")\n",
    "print(f\"  True Positives:  {total_tp}\")\n",
    "print(f\"  False Positives: {total_fp}\")\n",
    "print(f\"  False Negatives: {total_fn}\")\n",
    "print(f\"  Precision: {total_tp/(total_tp+total_fp):.4f}\")\n",
    "print(f\"  Recall: {total_tp/(total_tp+total_fn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find images with most errors\n",
    "images_by_fn = sorted(error_analysis.items(), \n",
    "                      key=lambda x: x[1]['fn'], reverse=True)\n",
    "\n",
    "print(\"\\nImages with most false negatives (missed detections):\")\n",
    "for img_id, stats in images_by_fn[:10]:\n",
    "    print(f\"  Image {img_id}: {stats['fn']} FN, {stats['gt_count']} GT\")\n",
    "\n",
    "images_by_fp = sorted(error_analysis.items(),\n",
    "                      key=lambda x: x[1]['fp'], reverse=True)\n",
    "\n",
    "print(\"\\nImages with most false positives (wrong detections):\")\n",
    "for img_id, stats in images_by_fp[:10]:\n",
    "    print(f\"  Image {img_id}: {stats['fp']} FP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synloc.visualization import draw_pitch, visualize_bev_predictions\n",
    "from synloc.data.camera import keypoint_to_world\n",
    "from PIL import Image\n",
    "\n",
    "def visualize_errors(img_id, coco_gt, results, data_root, threshold=0.3):\n",
    "    \"\"\"Visualize predictions vs ground truth for an image.\"\"\"\n",
    "    # Get image info and annotations\n",
    "    img_info = coco_gt.loadImgs(img_id)[0]\n",
    "    ann_ids = coco_gt.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_gt.loadAnns(ann_ids)\n",
    "    \n",
    "    # Get predictions for this image\n",
    "    preds = [r for r in results if r['image_id'] == img_id and r['score'] >= threshold]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Image view\n",
    "    img_path = data_root / 'test/images' / img_info['file_name']\n",
    "    img = Image.open(img_path)\n",
    "    axes[0].imshow(img)\n",
    "    \n",
    "    # Draw GT bboxes in blue\n",
    "    for ann in anns:\n",
    "        x, y, w, h = ann['bbox']\n",
    "        rect = plt.Rectangle((x, y), w, h, fill=False, \n",
    "                              edgecolor='blue', linewidth=2, linestyle='--')\n",
    "        axes[0].add_patch(rect)\n",
    "    \n",
    "    # Draw predictions in green\n",
    "    for pred in preds:\n",
    "        x, y, w, h = pred['bbox']\n",
    "        rect = plt.Rectangle((x, y), w, h, fill=False, \n",
    "                              edgecolor='green', linewidth=2)\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].text(x, y-5, f\"{pred['score']:.2f}\", color='green', fontsize=8)\n",
    "    \n",
    "    axes[0].set_title(f\"Image {img_id}: {len(anns)} GT (blue), {len(preds)} Pred (green)\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # BEV view\n",
    "    gt_positions = np.array([ann['position_on_pitch'][:2] for ann in anns])\n",
    "    \n",
    "    # Project predictions to BEV\n",
    "    pred_positions = []\n",
    "    camera_matrix = torch.tensor(img_info['camera_matrix'], dtype=torch.float32)\n",
    "    undist_poly = torch.tensor(img_info['undist_poly'], dtype=torch.float32)\n",
    "    \n",
    "    for pred in preds:\n",
    "        kpts = np.array(pred['keypoints']).reshape(-1, 3)\n",
    "        # Use pelvis_ground (index 1)\n",
    "        kpt = torch.tensor(kpts[1, :2], dtype=torch.float32).unsqueeze(0)\n",
    "        # Normalize\n",
    "        kpt_norm = (kpt - torch.tensor([(img_info['width']-1)/2, (img_info['height']-1)/2])) / img_info['width']\n",
    "        world = keypoint_to_world(camera_matrix, undist_poly, kpt_norm)\n",
    "        pred_positions.append(world[0, :2].numpy())\n",
    "    \n",
    "    if pred_positions:\n",
    "        pred_positions = np.array(pred_positions)\n",
    "    else:\n",
    "        pred_positions = np.array([]).reshape(0, 2)\n",
    "    \n",
    "    visualize_bev_predictions(\n",
    "        pred_positions, gt_positions,\n",
    "        ax=axes[1],\n",
    "        title='BEV View'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images with errors\n",
    "for img_id, stats in images_by_fn[:3]:\n",
    "    print(f\"\\nImage {img_id}: {stats['fn']} FN, {stats['fp']} FP\")\n",
    "    try:\n",
    "        visualize_errors(img_id, coco_gt, results, DATA_ROOT, threshold=optimal_threshold)\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Localization Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute localization errors for matched detections\n",
    "loc_errors = []\n",
    "\n",
    "for e in coco_eval.evalImgs:\n",
    "    if e is None:\n",
    "        continue\n",
    "    \n",
    "    img_id = e['image_id']\n",
    "    dt_matches = e['dtMatches'][iou_idx]\n",
    "    dt_scores = np.array(e['dtScores'])\n",
    "    \n",
    "    # Get image info\n",
    "    img_info = coco_gt.loadImgs(int(img_id))[0]\n",
    "    camera_matrix = torch.tensor(img_info['camera_matrix'], dtype=torch.float32)\n",
    "    undist_poly = torch.tensor(img_info['undist_poly'], dtype=torch.float32)\n",
    "    \n",
    "    # Get GT positions\n",
    "    gt_anns = coco_gt.loadAnns(coco_gt.getAnnIds(imgIds=int(img_id)))\n",
    "    gt_id_to_pos = {ann['id']: ann['position_on_pitch'][:2] for ann in gt_anns}\n",
    "    \n",
    "    # Get detections for this image\n",
    "    img_dets = [r for r in results if r['image_id'] == img_id]\n",
    "    \n",
    "    for i, (gt_id, score) in enumerate(zip(dt_matches, dt_scores)):\n",
    "        if gt_id > 0 and score >= optimal_threshold:\n",
    "            # Project detection to world\n",
    "            det = img_dets[i] if i < len(img_dets) else None\n",
    "            if det is None:\n",
    "                continue\n",
    "            \n",
    "            kpts = np.array(det['keypoints']).reshape(-1, 3)\n",
    "            kpt = torch.tensor(kpts[1, :2], dtype=torch.float32).unsqueeze(0)\n",
    "            kpt_norm = (kpt - torch.tensor([(img_info['width']-1)/2, (img_info['height']-1)/2])) / img_info['width']\n",
    "            \n",
    "            try:\n",
    "                world = keypoint_to_world(camera_matrix, undist_poly, kpt_norm)\n",
    "                pred_pos = world[0, :2].numpy()\n",
    "                gt_pos = np.array(gt_id_to_pos[int(gt_id)])\n",
    "                \n",
    "                error = np.linalg.norm(pred_pos - gt_pos)\n",
    "                loc_errors.append(error)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "loc_errors = np.array(loc_errors)\n",
    "print(f\"Localization errors for {len(loc_errors)} matched detections:\")\n",
    "print(f\"  Mean: {loc_errors.mean():.3f} m\")\n",
    "print(f\"  Median: {np.median(loc_errors):.3f} m\")\n",
    "print(f\"  Std: {loc_errors.std():.3f} m\")\n",
    "print(f\"  90th percentile: {np.percentile(loc_errors, 90):.3f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(loc_errors, bins=50, edgecolor='black')\n",
    "plt.axvline(x=1.0, color='r', linestyle='--', label='LocSim tau=1m')\n",
    "plt.xlabel('Localization Error (meters)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Localization Errors')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "thresholds = np.linspace(0, 5, 100)\n",
    "fractions = [np.mean(loc_errors <= t) for t in thresholds]\n",
    "plt.plot(thresholds, fractions)\n",
    "plt.axvline(x=1.0, color='r', linestyle='--', label='LocSim tau=1m')\n",
    "plt.xlabel('Distance Threshold (meters)')\n",
    "plt.ylabel('Fraction of Detections')\n",
    "plt.title('Cumulative Localization Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ablation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation(model_variant, input_size, checkpoint_path=None):\n",
    "    \"\"\"Run evaluation with specific settings.\"\"\"\n",
    "    # Create model\n",
    "    model = YOLOXPose(\n",
    "        variant=model_variant,\n",
    "        num_keypoints=2,\n",
    "        input_size=input_size\n",
    "    )\n",
    "    \n",
    "    if checkpoint_path and Path(checkpoint_path).exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dataset\n",
    "    test_dataset = SynLocDataset(\n",
    "        ann_file=str(DATA_ROOT / 'test/annotations.json'),\n",
    "        img_dir=str(DATA_ROOT / 'test/images'),\n",
    "        transforms=get_val_transforms(input_size[0]),\n",
    "        input_size=input_size\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        collate_fn=SynLocDataset.collate_fn\n",
    "    )\n",
    "    \n",
    "    # Run inference\n",
    "    results = run_inference(model, test_loader, device=device)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_predictions(\n",
    "        gt_file=str(DATA_ROOT / 'test/annotations.json'),\n",
    "        results=results,\n",
    "        position_from_keypoint_index=1\n",
    "    )\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example ablation: compare input sizes\n",
    "# Note: You would need checkpoints trained at each size\n",
    "\n",
    "ablation_results = {\n",
    "    'current': metrics  # Already computed\n",
    "}\n",
    "\n",
    "# Uncomment to run ablation\n",
    "# for size in [(640, 640), (960, 960)]:\n",
    "#     checkpoint = CHECKPOINT_DIR / f'model_{size[0]}.pth'\n",
    "#     if checkpoint.exists():\n",
    "#         ablation_results[f'{size[0]}x{size[1]}'] = run_ablation(\n",
    "#             config['model_variant'], size, checkpoint\n",
    "#         )\n",
    "\n",
    "print(\"\\nAblation Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Setting':<20} {'mAP-LocSim':>12} {'Precision':>12} {'Recall':>12}\")\n",
    "print(\"-\" * 60)\n",
    "for name, m in ablation_results.items():\n",
    "    print(f\"{name:<20} {m['mAP_locsim']:>12.4f} {m['precision']:>12.4f} {m['recall']:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Evaluation complete! Key findings:\n",
    "\n",
    "1. **mAP-LocSim**: {metrics.get('mAP_locsim', 'N/A'):.4f}\n",
    "2. **Best F1**: {metrics.get('f1', 'N/A'):.4f} @ threshold {metrics.get('score_threshold', 'N/A'):.3f}\n",
    "3. **Mean localization error**: {loc_errors.mean():.3f} m\n",
    "\n",
    "Next steps:\n",
    "- Proceed to `04_submission.ipynb` to generate challenge submission\n",
    "- Consider improvements based on error analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
